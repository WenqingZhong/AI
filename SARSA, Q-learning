import numpy as np
actions = ['<', '>', '^', 'v']

def Qvalue(state, action, values, p, gamma):
  """
  Compute the Q-value for the given state-action pair,
  given a set of values for the problem, with successful transition
  probability p and discount factor gamma.
  """
  gV = gamma*values
  pn = (1-p)/2

  # Deal with the possible transitions for the start state separately
  if state == (3,0): 
    if action == '>':
      return p*(-100+gV[(3,0)]) + pn*(-1+gV[(3,0)]) + pn*(-1+gV[(2,0)])
    elif action == '^':
      return p*(-1+gV[(2,0)]) + pn*(-1+gV[(3,0)]) + pn*(-100+gV[(3,0)])
    elif action == 'v':
      return p*(-1+gV[(3,0)]) + pn*(-1+gV[(3,0)]) + pn*(-100+gV[(3,0)])
    else:
      return p*(-1+gV[(3,0)]) + pn*(-1+gV[(3,0)]) + pn*(-1+gV[(2,0)])

  # All possible successor states
  i,j = state
  left = (i,max(j-1,0))
  right = (i,min(j+1,11))
  up = (max(i-1,0),j)
  down = (min(i+1,3),j)

  # Only way we get to the cliff is if we go down from states right above
  if down[0] == 3 and 0 < down[1] < 11:
    down = (3,0)
    dr = -100
  else:
    dr = -1

  # Q-value computation
  if action == '<':
    return p*(-1+gV[left]) + pn*(-1+gV[up]) + pn*(dr+gV[down])
  elif action == '>':
    return p*(-1+gV[right]) + pn*(-1+gV[up]) + pn*(dr+gV[down])
  elif action == '^':
    return p*(-1+gV[up]) + pn*(-1+gV[left]) + pn*(-1+gV[right])
  else:
    return p*(dr+gV[down]) + pn*(-1+gV[left]) + pn*(-1+gV[right])
    
    
def value_iteration(values, p, gamma, threshold=1e-6):
  # Given an initial array of values, returns array of values optimal within threshold
  max_diff = float("inf")
  while max_diff > threshold:
    new_values = -float("inf") * np.ones((4,12))
    new_values[3,1:] = 0
    # YOUR CODE HERE
    new_values[3,0]=max(Qvalue((3,0),'<',values,p,gamma),Qvalue((3,0),'>',values,p,gamma),Qvalue((3,0),'^',values,p,gamma),Qvalue((3,0),'V',values,p,gamma))
    values[3,0]=new_values[3,0]

    temp_diff=-float("inf")
    for i in range(0,3):
      for j in range(0,12):
        new_values[i,j]=max(Qvalue((i,j),'<',values,p,gamma),Qvalue((i,j),'>',values,p,gamma),Qvalue((i,j),'^',values,p,gamma),Qvalue((i,j),'V',values,p,gamma))
        one_diff=abs(values[i,j]-new_values[i,j])
        if (one_diff>temp_diff):
           temp_diff=one_diff

    max_diff=temp_diff

    for i in range(0,3):
      for j in range(0,12):
        values[i,j]=new_values[i,j]

  return values
  
def extract_policy(values, p, gamma):
  # Extract the optimal policy associated with the given optimal values
  policy = np.empty(values.shape, dtype=object)
  for i in range(4):
    for j in range(12):
      best_value = -float("inf")
      for a in actions:
        new_value = Qvalue((i,j), a, values, p, gamma)
        if new_value > best_value:
          best_value = new_value
          policy[i,j] = a
  return policy

def optimal_values_policy(p, gamma):
  # Find and show the optimal values and policy for the given parameters
  values = value_iteration(np.zeros((4,12)), p, gamma)
  policy = extract_policy(values, p, gamma)
  print(np.round(values,2))
  print("")
  policy[3,1:11] = 'C'
  policy[3,11] = 'G'  
  print(policy)
  
  
  
  import random

def epsilon_greedy_action(Qvalues, state, epsilon):
  # Explore a random action from state with probability epsilon
  # Otherwise, greedily choose the best action
  # Qvalues is a dictionary that looks like {(state, action): q-value}
  # YOUR CODE HERE
  qvalues=list(Qvalues.values())
  qstate_actions=list(Qvalues.keys())
  maxq=-float("inf")
  max_action= None
  for i in range(len(qvalues)):
    if (qstate_actions[i][0]==state):
      if (qvalues[i]>maxq):
          maxq=qvalues[i]
          max_action=qstate_actions[i]

  if random.random() < epsilon:
    random_action=random.choice(qstate_actions)
    return random_action[1]
  else:
    return max_action[1]

def step(state, action, p):
  # Return successor state and reward upon taking action from state
  i,j = state

  if action == '<':
    if random.random() < p:
      state = (i,max(j-1,0))
    else:
      state = random.choice([(max(i-1,0),j), (min(i+1,3),j)])
  elif action == '>':
    if random.random() < p:
      state = (i,min(j+1,11))
    else:
      state = random.choice([(max(i-1,0),j), (min(i+1,3),j)])
  elif action == '^':
    if random.random() < p:
      state = (max(i-1,0),j)
    else:
      state = random.choice([(i,max(j-1,0)), (i,min(j+1,11))])
  else:
    if random.random() < p:
      state = (min(i+1,3),j)
    else:
      state = random.choice([(i,max(j-1,0)), (i,min(j+1,11))])    
  
  if state[0] == 3 and 0 < state[1] < 11:
    reward = -100
  else:
    reward = -1
  return state, reward
  
  
  
def SARSA(Qvalues, p, gamma, alpha, epsilon, episodes=100000):
  # SARSA temporal difference learning using initial Qvalues and given parameters
  # Returns a learned policy (numpy 2d array)
  for i in range(episodes):
    state = (3,0)
    action = epsilon_greedy_action(Qvalues, state, epsilon)
    while state == (3,0) or state[0] != 3:
      next_state, reward = step(state, action, p)
      next_action = epsilon_greedy_action(Qvalues, next_state, epsilon)
      target = Qvalues[(next_state, next_action)]
      Qvalues[(state, action)] += alpha * (reward + gamma*target - Qvalues[(state, action)])
      state = next_state
      action = next_action
  policy = extract_policy(Qvalues)
  return policy

def extract_policy(Qvalues):
  # Extract the optimal policy associated with the given Q-values
  policy = np.empty((4,12), dtype=object)
  for i in range(4):
    for j in range(12):
      policy[i,j] = epsilon_greedy_action(Qvalues, (i,j), 0)
  return policy
  
  
  def Qlearner(Qvalues, p, gamma, alpha, epsilon, episodes=100000):
  # Q-learning using initial Qvalues and given parameters
  # Returns a learned policy (numpy 2d array)  
  # YOUR CODE HERE
  for i in range(episodes):
    state = (3,0)
    action = epsilon_greedy_action(Qvalues, state, epsilon)
    while state == (3,0) or state[0] != 3:
      next_state, reward = step(state, action, p)
      #next_action = epsilon_greedy_action(Qvalues, next_state, epsilon)
      target_best_action= epsilon_greedy_action(Qvalues, next_state, 0)
      target = Qvalues[(next_state, target_best_action)]
      Qvalues[(state, action)] += alpha * (reward + gamma*target - Qvalues[(state, action)])
      state = next_state
      action = target_best_action
  policy = extract_policy(Qvalues)
  return policy

def TD_learn(p, gamma, alpha, epsilon):
  Qvalues = {((i,j),a): 0 for i in range(4) for j in range(12) for a in actions}
  policy = SARSA(Qvalues, p, gamma, alpha, epsilon)
  print("SARSA policy")
  policy[3,1:11] = 'C'
  policy[3,11] = 'G'    
  print(policy)
  
  Qvalues = {((i,j),a): 0 for i in range(4) for j in range(12) for a in actions}
  policy = Qlearner(Qvalues, p, gamma, alpha, epsilon)
  print("Q-learning policy")
  policy[3,1:11] = 'C'
  policy[3,11] = 'G'    
  print(policy)
  
p = 1
gamma = 1
alpha = 0.9
epsilon = 0.2
TD_learn(p, gamma, alpha, epsilon)
